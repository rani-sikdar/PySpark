{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxydnI/HNMBj5nJ838K2/z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rani-sikdar/PySpark/blob/main/pyspark_intermediate_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### working with dates and time"
      ],
      "metadata": {
        "id": "M5VDZBelCSgF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KqJgYf4-nDbp"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    current_date, current_timestamp, date_format, year, month, dayofmonth,\n",
        "    dayofweek, dayofyear, weekofyear, quarter, date_add, date_sub,\n",
        "    to_date, to_timestamp, datediff, months_between, next_day, last_day\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sess = SparkSession.builder.appName('demo').getOrCreate()\n",
        "sess"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "jrft6b35CC2P",
        "outputId": "3202539a-7c46-4253-b469-3dc1d867658f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7deedf6579b0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://c23fdd3de4e3:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>demo</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "current_date()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed7lqFi8CCz8",
        "outputId": "0887acb1-1d72-46a9-e98c-9ce229049596"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Column<'current_date()'>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = sess.range(1).withColumn(\"current_date\", current_date()).withColumn(\"current_timestamp\", current_timestamp())\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWX4FVrVCCxN",
        "outputId": "6d374a76-4c5e-45f8-939b-cdc6ee3ec241"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------+--------------------+\n",
            "| id|current_date|   current_timestamp|\n",
            "+---+------------+--------------------+\n",
            "|  0|  2025-09-25|2025-09-25 04:05:...|\n",
            "+---+------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = sess.range(1).withColumn(\"current_date\", current_date()).withColumn(\"current_timestamp\", current_timestamp())\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnGJCXZACCtp",
        "outputId": "80626bae-af65-4562-8b89-169c441c0ad7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------+-------------------------+\n",
            "|id |current_date|current_timestamp        |\n",
            "+---+------------+-------------------------+\n",
            "|0  |2025-09-25  |2025-09-25 04:06:17.79182|\n",
            "+---+------------+-------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dates = sess.createDataFrame([(\"2025-09-25\",)], [\"date_str\"])\n",
        "df_dates = df_dates.withColumn(\"date\", to_date(\"date_str\"))\n",
        "df_dates.select(\n",
        "    \"date\",\n",
        "    year(\"date\").alias(\"year\"),\n",
        "    month(\"date\").alias(\"month\"),\n",
        "    dayofmonth(\"date\").alias(\"day\"),\n",
        "    weekofyear(\"date\").alias(\"week\"),\n",
        "    quarter(\"date\").alias(\"quarter\")\n",
        ").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH1uAqLTCCqu",
        "outputId": "1e11792d-33c1-4e90-87b2-4031a3bfd5f4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+-----+---+----+-------+\n",
            "|      date|year|month|day|week|quarter|\n",
            "+----------+----+-----+---+----+-------+\n",
            "|2025-09-25|2025|    9| 25|  39|      3|\n",
            "+----------+----+-----+---+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dates.select(date_format(\"date\", \"yyyy/MM/dd\").alias(\"formatted_date\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1VB6rD-CCnf",
        "outputId": "60606528-8bbb-46e8-a2fc-ac735ec5c60c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+\n",
            "|formatted_date|\n",
            "+--------------+\n",
            "|    2025/09/25|\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dates.select(\n",
        "    next_day(\"date\", \"Sunday\").alias(\"next_sunday\"),\n",
        "    last_day(\"date\").alias(\"month_end\")\n",
        ").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHwpraN3CCkM",
        "outputId": "cf33d0f2-b744-417e-cd4d-ea5cbe003a65"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+\n",
            "|next_sunday| month_end|\n",
            "+-----------+----------+\n",
            "| 2025-09-28|2025-09-30|\n",
            "+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date, to_timestamp, unix_timestamp, from_unixtime"
      ],
      "metadata": {
        "id": "PM9eWF6_CChw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "date = [(\"2025-09-08 14:35:00\",)]\n",
        "df = sess.createDataFrame(date, [\"date_str\"])"
      ],
      "metadata": {
        "id": "O3xsSp-9CCeR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6L9z8xYENc8",
        "outputId": "1ae41509-6c3a-4dc9-dea9-a59fd2718c46"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|           date_str|\n",
            "+-------------------+\n",
            "|2025-09-08 14:35:00|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_converted = df.withColumn(\"as_date\", to_date(\"date_str\")) \\\n",
        "                 .withColumn(\"as_timestamp\", to_timestamp(\"date_str\"))\\\n",
        "                 .withColumn(\"as_timestamp_2\", to_timestamp(\"date_str\", \"dd/MM/yyyy hh:mm a\"))\\\n",
        "                 .withColumn(\"as_unix_timestamp\", unix_timestamp(\"date_str\"))\\\n",
        "                 .withColumn(\"as_human_timestamp\", from_unixtime(\"as_unix_timestamp\"))\n",
        "\n",
        "df_converted.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTbwWX_ECCa9",
        "outputId": "0a2d051d-b942-4498-cc0e-9df4975a867e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----------+-------------------+--------------+-----------------+-------------------+\n",
            "|date_str           |as_date   |as_timestamp       |as_timestamp_2|as_unix_timestamp|as_human_timestamp |\n",
            "+-------------------+----------+-------------------+--------------+-----------------+-------------------+\n",
            "|2025-09-08 14:35:00|2025-09-08|2025-09-08 14:35:00|NULL          |1757342100       |2025-09-08 14:35:00|\n",
            "+-------------------+----------+-------------------+--------------+-----------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### date arthmetic"
      ],
      "metadata": {
        "id": "i8IGw3mzFUll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date, date_add, date_sub"
      ],
      "metadata": {
        "id": "NIT19_q9CCXi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = sess.createDataFrame([(\"2025-09-08\",)], [\"date_str\"])\n",
        "df = df.withColumn(\"date\", to_date(\"date_str\"))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTdUFOSXCCUr",
        "outputId": "2f4667e1-578d-4e92-dade-55fd1f9a1f08"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|  date_str|      date|\n",
            "+----------+----------+\n",
            "|2025-09-08|2025-09-08|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_arithmetic = df.select(\n",
        "    \"date\",\n",
        "    date_add(\"date\", 7).alias(\"add_7_days\"),\n",
        "    date_sub(\"date\", 7).alias(\"minus_7_days\")\n",
        ")\n",
        "df_arithmetic.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzrmWEGkCCRP",
        "outputId": "9e966dfe-1f76-4b69-fccf-fd3140a97bef"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+------------+\n",
            "|      date|add_7_days|minus_7_days|\n",
            "+----------+----------+------------+\n",
            "|2025-09-08|2025-09-15|  2025-09-01|\n",
            "+----------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# datediff(end_date, start_date)"
      ],
      "metadata": {
        "id": "W6f46cnVCCOV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import datediff\n",
        "\n",
        "df2 = sess.createDataFrame([(\"2025-09-08\", \"2025-10-01\")], [\"start_date\", \"end_date\"])\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ko7JBzZCCKh",
        "outputId": "cf474cdc-0fa6-415b-8938-a8b8f6d8af1a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|start_date|  end_date|\n",
            "+----------+----------+\n",
            "|2025-09-08|2025-10-01|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df2.withColumn(\"start\", to_date(\"start_date\")) \\\n",
        "                 .withColumn(\"end\", to_date(\"end_date\"))\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNxH54rtKTMM",
        "outputId": "22d1d64e-fe41-4bd2-f1a2-dad200bfddf8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+----------+\n",
            "|start_date|  end_date|     start|       end|\n",
            "+----------+----------+----------+----------+\n",
            "|2025-09-08|2025-10-01|2025-09-08|2025-10-01|\n",
            "+----------+----------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df2 = df2.withColumn(\"days_between\", datediff(\"end\", \"start\"))\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnanVhy8CCD4",
        "outputId": "72c8942d-f3c0-4a27-f27b-48c6aaf84981"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+----------+------------+\n",
            "|start_date|  end_date|     start|       end|days_between|\n",
            "+----------+----------+----------+----------+------------+\n",
            "|2025-09-08|2025-10-01|2025-09-08|2025-10-01|          23|\n",
            "+----------+----------+----------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.select(\"start_date\", \"end_date\", \"days_between\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mV0JiEgCCA3",
        "outputId": "0168d257-8cc4-4df4-f771-4d0b9a70a002"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+------------+\n",
            "|start_date|  end_date|days_between|\n",
            "+----------+----------+------------+\n",
            "|2025-09-08|2025-10-01|          23|\n",
            "+----------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import last_day, next_day\n",
        "\n",
        "df3 = df2.select(\n",
        "    last_day(\"start_date\").alias(\"month_end\"),\n",
        "    next_day(\"start_date\", \"Friday\").alias(\"next_friday\")\n",
        ")\n",
        "df3.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdQhyZAvCB9O",
        "outputId": "36baa400-4dc8-4863-917d-99f08101bf84"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "| month_end|next_friday|\n",
            "+----------+-----------+\n",
            "|2025-09-30| 2025-09-12|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Use Case:\n",
        "  ETL Pipelines – Partitioning and Filtering Data\n",
        "\n",
        "Scenario:\n",
        "  You receive daily logs in a data lake (/logs/2025/09/08/) and want to process only yesterday’s data.\n",
        "\n",
        "Solution:\n",
        "\"\"\"\n",
        "from pyspark.sql.functions import current_date, date_sub\n",
        "\n",
        "# Filter only yesterday's data\n",
        "yesterday = date_sub(current_date(), 1)\n",
        "logs_df = sess.read.parquet(\"/content/house-price.parquet\")  # sample log attached here\n",
        "logs_df = logs_df.filter(logs_df.event_date == yesterday)\n"
      ],
      "metadata": {
        "id": "EOTvIq0DCB6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Use Case:\n",
        "  Optimizes ETL by processing only the required partition.\n",
        "\n",
        "Scenario:\n",
        "  Time-Series Analytics – Trend Over Time\n",
        "  You want to calculate daily active users (DAU) for the last 30 days.\n",
        "\n",
        "Solution:\n",
        "\"\"\"\n",
        "from pyspark.sql.functions import current_date, date_sub\n",
        "\n",
        "start_date = date_sub(current_date(), 30)  # last 30 days date\n",
        "dau_df = logs_df.filter(logs_df.login_date >= start_date) \\\n",
        "                  .groupBy(\"login_date\") \\\n",
        "                  .count() \\\n",
        "                  .orderBy(\"login_date\")\n"
      ],
      "metadata": {
        "id": "Y3-XlrvfCB1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Use Case:\n",
        "  Rolling windows for reporting, dashboards, and analytics.\n",
        "\n",
        "Scenario:\n",
        "  SLA Monitoring – Calculating Processing Time\n",
        "  You want to check how long each job took to complete.\n",
        "\n",
        "Solution:\n",
        "\"\"\"\n",
        "from pyspark.sql.functions import unix_timestamp\n",
        "\n",
        "jobs_df = sess.createDataFrame([\n",
        "    (\"2025-09-08 10:00:00\", \"2025-09-08 14:30:00\"),\n",
        "    (\"2025-09-08 12:15:00\", \"2025-09-08 13:00:00\")\n",
        "], [\"start_time\", \"end_time\"])\n",
        "\n",
        "jobs_df = jobs_df.withColumn(\"duration_minutes\",\n",
        "    (unix_timestamp(\"end_time\") - unix_timestamp(\"start_time\")) / 60\n",
        ")\n",
        "jobs_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_oYFeJCCBx2",
        "outputId": "682a0aed-6548-4afd-9047-f586f35acdf5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------------------+----------------+\n",
            "|         start_time|           end_time|duration_minutes|\n",
            "+-------------------+-------------------+----------------+\n",
            "|2025-09-08 10:00:00|2025-09-08 14:30:00|           270.0|\n",
            "|2025-09-08 12:15:00|2025-09-08 13:00:00|            45.0|\n",
            "+-------------------+-------------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Use Case:\n",
        "  SLA compliance, performance monitoring, and alerts.\n",
        "\n",
        "Scenario:\n",
        "   Subscription Billing – Next Renewal Date.\n",
        "   Each user has a subscription date. You need to calculate their next billing date (every 30 days).\n",
        "\n",
        "Solution:\n",
        "\"\"\"\n",
        "from pyspark.sql.functions import to_date, date_add\n",
        "\n",
        "dff = sess.createDataFrame([\n",
        "    (\"2025-09-08\",), (\"2025-09-10\",)\n",
        "], [\"subscription_date\"])\n",
        "\n",
        "# dff.show()\n",
        "\n",
        "dff = dff.withColumn(\"next_billing_date\", date_add(\"subscription_date\", 30))\n",
        "dff.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83nkOCJoCBuO",
        "outputId": "66c1d511-2a46-439a-ec7d-898efcb00a80"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------------+\n",
            "|subscription_date|next_billing_date|\n",
            "+-----------------+-----------------+\n",
            "|       2025-09-08|       2025-10-08|\n",
            "|       2025-09-10|       2025-10-10|\n",
            "+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Use Case:\n",
        "  Data Retention – Archiving Old Data\n",
        "\n",
        "Scenario:\n",
        "   Archive data older than 6 months.\n",
        "\n",
        "Solution:\n",
        "\"\"\"\n",
        "\n",
        "from pyspark.sql.functions import add_months, current_date\n",
        "\n",
        "cutoff_date = add_months(current_date(), -6)\n",
        "archived_df = dff.filter(dff.subscription_date < cutoff_date)\n",
        "archived_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSC4dpDCNekh",
        "outputId": "52ce48d1-1107-419c-c5d8-16713e5dc8ab"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------------+\n",
            "|subscription_date|next_billing_date|\n",
            "+-----------------+-----------------+\n",
            "+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sess"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "Lowk-o8bN5s4",
        "outputId": "8e727968-9caa-4677-a804-daf4c0b22cc6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7e75240bbbc0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e05d95202dbb:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>demo</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"East\", \"ProductA\", 100),\n",
        "    (\"East\", \"ProductB\", 200),\n",
        "    (\"West\", \"ProductA\", 150),\n",
        "    (\"West\", \"ProductB\", 300),\n",
        "    (\"West\", \"ProductA\", 250)\n",
        "]\n",
        "df = sess.createDataFrame(data, [\"region\", \"product\",\"sales\"])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkCcMVWieQrg",
        "outputId": "5a05fd9a-5c28-4f11-c825-40178178f56a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----+\n",
            "|region| product|sales|\n",
            "+------+--------+-----+\n",
            "|  East|ProductA|  100|\n",
            "|  East|ProductB|  200|\n",
            "|  West|ProductA|  150|\n",
            "|  West|ProductB|  300|\n",
            "|  West|ProductA|  250|\n",
            "+------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"region\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5vOWmJbe0cc",
        "outputId": "6a6dfabd-0f6b-4efe-d929-d47605ef4cb9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|region|count|\n",
            "+------+-----+\n",
            "|  East|    2|\n",
            "|  West|    3|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"region\").sum(\"sales\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHewb-Jke4r2",
        "outputId": "2dc92f6c-9748-4e96-8b0e-24c6a3d261b3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+\n",
            "|region|sum(sales)|\n",
            "+------+----------+\n",
            "|  East|       300|\n",
            "|  West|       700|\n",
            "+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, avg, count, max, min, mean, countDistinct"
      ],
      "metadata": {
        "id": "SYU3mXC9fXYY"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"region\", \"product\").agg(\n",
        "    sum(\"sales\").alias(\"total_sales\"),\n",
        "    avg(\"sales\").alias(\"avg_sales\"),\n",
        ").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0brNASBqfPhK",
        "outputId": "90cbb3aa-23cb-42ff-dbfe-ddff00ac0d16"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----------+---------+\n",
            "|region| product|total_sales|avg_sales|\n",
            "+------+--------+-----------+---------+\n",
            "|  East|ProductB|        200|    200.0|\n",
            "|  East|ProductA|        100|    100.0|\n",
            "|  West|ProductA|        400|    200.0|\n",
            "|  West|ProductB|        300|    300.0|\n",
            "+------+--------+-----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"region\").agg(\n",
        "    countDistinct(\"product\").alias(\"distinct_products\"),\n",
        "    max(\"sales\").alias(\"max_sales\"),\n",
        "    min(\"sales\").alias(\"min_sales\"),\n",
        "    avg(\"sales\").alias(\"mean_sales\")\n",
        ").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8HyR2FYfa5s",
        "outputId": "cfefa4fc-894c-4579-dc6f-a1a09da83f3c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------------+---------+---------+------------------+\n",
            "|region|distinct_products|max_sales|min_sales|        mean_sales|\n",
            "+------+-----------------+---------+---------+------------------+\n",
            "|  East|                2|      200|      100|             150.0|\n",
            "|  West|                2|      300|      150|233.33333333333334|\n",
            "+------+-----------------+---------+---------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"region\").agg(\n",
        "    {\"sales\": \"sum\", \"sales\": \"avg\"}\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATmqzxXdfyHx",
        "outputId": "fd8ba65a-500b-4ece-81d8-6690273579aa"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------------+\n",
            "|region|        avg(sales)|\n",
            "+------+------------------+\n",
            "|  East|             150.0|\n",
            "|  West|233.33333333333334|\n",
            "+------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"region\").sum(\"sales\").orderBy(sum(\"sales\"), ascending=False).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9jFlzc_f5QZ",
        "outputId": "fa69ac6e-9188-4a13-c6a7-b655c88ceaf6"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+\n",
            "|region|sum(sales)|\n",
            "+------+----------+\n",
            "|  West|       700|\n",
            "|  East|       300|\n",
            "+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.toPandas().tail(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "w3ujl7biixVi",
        "outputId": "4801c761-3343-4168-cb54-baf7e012f104"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  region   product  sales\n",
              "4   West  ProductA    250"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e66be1b8-880b-4cf7-a6b7-688d7d9f4f64\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>region</th>\n",
              "      <th>product</th>\n",
              "      <th>sales</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>West</td>\n",
              "      <td>ProductA</td>\n",
              "      <td>250</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e66be1b8-880b-4cf7-a6b7-688d7d9f4f64')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e66be1b8-880b-4cf7-a6b7-688d7d9f4f64 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e66be1b8-880b-4cf7-a6b7-688d7d9f4f64');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"region\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"West\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"ProductA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sales\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 250,\n        \"max\": 250,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          250\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.toPandas().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JKxBkrQjGr2",
        "outputId": "4631cd8b-0de9-4d68-8aca-053576d81c91"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYGrC9yijnLO",
        "outputId": "b6797c9d-e970-4f9b-851e-2af92a8f9d75"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### window functions"
      ],
      "metadata": {
        "id": "TTILZB96MA7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank, dense_rank, row_number, col, to_date\n"
      ],
      "metadata": {
        "id": "ye8Z5rEvjdK-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"East\", \"ProductA\", 100),\n",
        "    (\"East\", \"ProductB\", 200),\n",
        "    (\"West\", \"ProductA\", 150),\n",
        "    (\"West\", \"ProductB\", 300),\n",
        "    (\"West\", \"ProductA\", 250)\n",
        "]\n",
        "df = sess.createDataFrame(data, [\"region\", \"product\", \"sales\"])\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnBKKP8eL6NB",
        "outputId": "14b44715-b62f-4d7d-d89a-2ac5061401ac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----+\n",
            "|region| product|sales|\n",
            "+------+--------+-----+\n",
            "|  East|ProductA|  100|\n",
            "|  East|ProductB|  200|\n",
            "|  West|ProductA|  150|\n",
            "|  West|ProductB|  300|\n",
            "|  West|ProductA|  250|\n",
            "+------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_spec = Window.partitionBy(\"region\").orderBy(col(\"sales\").desc())"
      ],
      "metadata": {
        "id": "G4968JeuQ6Ib"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKfOrkmMS1cd",
        "outputId": "3a40d47c-2909-48b8-f28d-d45cb38cfd69"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.window.WindowSpec at 0x7deef409a870>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn(\"rank\", rank().over(window_spec)).show()   # rank"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQfdDfQ_TLI-",
        "outputId": "6a2f4218-ffac-4c4b-c424-dbb73f31a2a9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----+----+\n",
            "|region| product|sales|rank|\n",
            "+------+--------+-----+----+\n",
            "|  East|ProductB|  200|   1|\n",
            "|  East|ProductA|  100|   2|\n",
            "|  West|ProductB|  300|   1|\n",
            "|  West|ProductA|  250|   2|\n",
            "|  West|ProductA|  150|   3|\n",
            "+------+--------+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn(\"dense_rank\", dense_rank().over(window_spec)).show()   # dense rank"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jkbh7wUsTRQs",
        "outputId": "4613a7c9-6ff7-4792-e4e9-6ad5f1a79c7e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----+----------+\n",
            "|region| product|sales|dense_rank|\n",
            "+------+--------+-----+----------+\n",
            "|  East|ProductB|  200|         1|\n",
            "|  East|ProductA|  100|         2|\n",
            "|  West|ProductB|  300|         1|\n",
            "|  West|ProductA|  250|         2|\n",
            "|  West|ProductA|  150|         3|\n",
            "+------+--------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn(\"row_num\", row_number().over(window_spec)).show()   # row number"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9ijuh0NS298",
        "outputId": "e596634e-4c40-47c2-b1b5-0711321259cd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----+-------+\n",
            "|region| product|sales|row_num|\n",
            "+------+--------+-----+-------+\n",
            "|  East|ProductB|  200|      1|\n",
            "|  East|ProductA|  100|      2|\n",
            "|  West|ProductB|  300|      1|\n",
            "|  West|ProductA|  250|      2|\n",
            "|  West|ProductA|  150|      3|\n",
            "+------+--------+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 1. Top N Products per Region\n",
        "\n",
        "Scenario: Get the top 2 best-selling products in each region based on sales.\n",
        "\n",
        "Use Case: E-commerce dashboards, sales reports.\n",
        "\"\"\"\n",
        "window_spec = Window.partitionBy(\"region\").orderBy(col(\"sales\").desc())\n",
        "\n",
        "top_n_products = df.withColumn(\"row_num\", row_number().over(window_spec)).filter(col(\"row_num\") <= 2)\n",
        "\n",
        "top_n_products.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQaLhNACVBDT",
        "outputId": "f99ca629-0035-49d2-a417-c7c6bdd5f7af"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----+-------+\n",
            "|region| product|sales|row_num|\n",
            "+------+--------+-----+-------+\n",
            "|  East|ProductB|  200|      1|\n",
            "|  East|ProductA|  100|      2|\n",
            "|  West|ProductB|  300|      1|\n",
            "|  West|ProductA|  250|      2|\n",
            "+------+--------+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 2. Customer Retention – Latest Purchase Per Customer\n",
        "\n",
        "Scenario: You have multiple transactions per customer and want to find the latest transaction.\n",
        "\n",
        "Use Case: CRM analytics, churn prediction, personalized marketing.\n",
        "\"\"\"\n",
        "transactions = [\n",
        "    (1, \"2025-09-01\", 100),\n",
        "    (1, \"2025-09-05\", 150),\n",
        "    (2, \"2025-09-02\", 200),\n",
        "    (2, \"2025-09-06\", 300)\n",
        "]\n",
        "df_txn = sess.createDataFrame(transactions, [\"customer_id\", \"purchase_date\", \"amount\"])\n",
        "\n",
        "window_spec = Window.partitionBy(\"customer_id\").orderBy(col(\"purchase_date\").desc())\n",
        "\n",
        "latest_transaction = df_txn.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
        "                   .filter(col(\"rn\") == 1)\n",
        "\n",
        "latest_transaction.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUlZpHn0P0k8",
        "outputId": "06d768b3-f61f-4304-9355-cee2cfbdb658"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+------+---+\n",
            "|customer_id|purchase_date|amount| rn|\n",
            "+-----------+-------------+------+---+\n",
            "|          1|   2025-09-05|   150|  1|\n",
            "|          2|   2025-09-06|   300|  1|\n",
            "+-----------+-------------+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"3. Ranking Customers by Spending\n",
        "\n",
        "Scenario: Rank customers within each region based on total spend.\n",
        "\n",
        "Use Case: Loyalty programs, premium customer identification.\n",
        "\"\"\"\n",
        "from pyspark.sql.functions import sum as sum_\n",
        "\n",
        "# aggregate\n",
        "customer_spend = df.groupBy(\"region\", \"product\").agg(sum_(\"sales\").alias(\"total_sales\"))\n",
        "\n",
        "window_spec = Window.partitionBy(\"region\").orderBy(col(\"total_sales\").desc())\n",
        "\n",
        "ranked_customers = customer_spend.withColumn(\"rank\", rank().over(window_spec))\n",
        "ranked_customers.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YC9wz2_QVZt",
        "outputId": "2a6c5d02-c677-4987-8872-18ff83ef039c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----------+----+\n",
            "|region| product|total_sales|rank|\n",
            "+------+--------+-----------+----+\n",
            "|  East|ProductB|        200|   1|\n",
            "|  East|ProductA|        100|   2|\n",
            "|  West|ProductA|        400|   1|\n",
            "|  West|ProductB|        300|   2|\n",
            "+------+--------+-----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1ppK99DRxhC",
        "outputId": "53281c86-3879-4809-c40b-1ce543e59ec7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----+\n",
            "|region| product|sales|\n",
            "+------+--------+-----+\n",
            "|  East|ProductA|  100|\n",
            "|  East|ProductB|  200|\n",
            "|  West|ProductA|  150|\n",
            "|  West|ProductB|  300|\n",
            "|  West|ProductA|  250|\n",
            "+------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 4. Running Totals (Cumulative Sums)\n",
        "\n",
        "Scenario: Calculate cumulative sales per region over time.\n",
        "\n",
        "Use Case: Revenue growth trends, inventory planning.\n",
        "\"\"\"\n",
        "from pyspark.sql.functions import sum as sum_\n",
        "\n",
        "window_spec = Window.partitionBy(\"region\").orderBy(\"sales\") \\\n",
        "                    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "df_running = df.withColumn(\"cumulative_sales\", sum_(\"sales\").over(window_spec))\n",
        "df_running.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8IFxcXLQdiP",
        "outputId": "3073ece1-1dca-48e8-bcac-67c8351e3bf5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----+----------------+\n",
            "|region| product|sales|cumulative_sales|\n",
            "+------+--------+-----+----------------+\n",
            "|  East|ProductA|  100|             100|\n",
            "|  East|ProductB|  200|             300|\n",
            "|  West|ProductA|  150|             150|\n",
            "|  West|ProductA|  250|             400|\n",
            "|  West|ProductB|  300|             700|\n",
            "+------+--------+-----+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = [\n",
        "    (1, \"2025-09-01\", 100),\n",
        "    (1, \"2025-09-05\", 150),\n",
        "    (2, \"2025-09-02\", 200),\n",
        "    (2, \"2025-09-06\", 300)\n",
        "]\n",
        "df_txn = sess.createDataFrame(transactions, [\"customer_id\", \"purchase_date\", \"amount\"])\n"
      ],
      "metadata": {
        "id": "jxIdPO33ST_z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 5. Identifying First & Last Events (Sessionization)\n",
        "\n",
        "Scenario: Identify the first login and last login per user.\n",
        "\n",
        "Use Case: User session analysis, fraud detection.\n",
        "\"\"\"\n",
        "from pyspark.sql.functions import first, last\n",
        "\n",
        "window_spec = Window.partitionBy(\"customer_id\").orderBy(\"purchase_date\")\n",
        "\n",
        "dff = df_txn.withColumn(\"first_txn\", first(\"purchase_date\").over(window_spec)) \\\n",
        "               .withColumn(\"last_txn\", last(\"purchase_date\").over(window_spec))\n",
        "\n",
        "dff.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TeBX7yMQwzU",
        "outputId": "36a6a9f6-b2a3-4842-994e-266aed53489a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+------+----------+----------+\n",
            "|customer_id|purchase_date|amount| first_txn|  last_txn|\n",
            "+-----------+-------------+------+----------+----------+\n",
            "|          1|   2025-09-01|   100|2025-09-01|2025-09-01|\n",
            "|          1|   2025-09-05|   150|2025-09-01|2025-09-05|\n",
            "|          2|   2025-09-02|   200|2025-09-02|2025-09-02|\n",
            "|          2|   2025-09-06|   300|2025-09-02|2025-09-06|\n",
            "+-----------+-------------+------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = [(\"A\", 1), (\"B\", 2), (\"C\", 3)]\n",
        "data2 = [(\"A\", \"Apple\"), (\"B\", \"Banana\"), (\"D\", \"Dates\")]\n",
        "\n",
        "df1 = sess.createDataFrame(data1, [\"id\",\"value\"])\n",
        "df2 = sess.createDataFrame(data2, [\"id\",\"name\"])"
      ],
      "metadata": {
        "id": "aKIN2heJSly9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.show(), df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZmfkPwna7NL",
        "outputId": "8d95eb5c-109d-4e38-c446-e6fc29d8be94"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "| id|value|\n",
            "+---+-----+\n",
            "|  A|    1|\n",
            "|  B|    2|\n",
            "|  C|    3|\n",
            "+---+-----+\n",
            "\n",
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  A| Apple|\n",
            "|  B|Banana|\n",
            "|  D| Dates|\n",
            "+---+------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inner_join = df1.join(df2, on=\"id\",how=\"inner\")\n",
        "inner_join.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q53f35oKaQYV",
        "outputId": "d87f636a-6cbe-4c3a-deab-20cd271bc74d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+\n",
            "| id|value|  name|\n",
            "+---+-----+------+\n",
            "|  A|    1| Apple|\n",
            "|  B|    2|Banana|\n",
            "+---+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "left_join = df1.join(df2, on=\"id\", how=\"left\")\n",
        "left_join.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqwR0szXam7c",
        "outputId": "5dd3af41-38bc-41c8-dc53-c976f9880c40"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+\n",
            "| id|value|  name|\n",
            "+---+-----+------+\n",
            "|  A|    1| Apple|\n",
            "|  B|    2|Banana|\n",
            "|  C|    3|  NULL|\n",
            "+---+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "right_join = df1.join(df2, on=\"id\", how=\"right\")\n",
        "right_join.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d11wmtXia3JC",
        "outputId": "932b6ef5-484a-4203-dd26-f06598a81260"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+\n",
            "| id|value|  name|\n",
            "+---+-----+------+\n",
            "|  A|    1| Apple|\n",
            "|  B|    2|Banana|\n",
            "|  D| NULL| Dates|\n",
            "+---+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_outer = df1.join(df2, on=\"id\", how=\"outer\")\n",
        "full_outer.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1t248EKbN05",
        "outputId": "d44d6e87-da81-42f7-aff3-005ca25dbdb1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+\n",
            "| id|value|  name|\n",
            "+---+-----+------+\n",
            "|  A|    1| Apple|\n",
            "|  B|    2|Banana|\n",
            "|  C|    3|  NULL|\n",
            "|  D| NULL| Dates|\n",
            "+---+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Broadcast Joins in PySpark\n",
        "A Broadcast Join is a special type of join in PySpark designed to speed up joins when one of the DataFrames is small enough to fit in memory on all worker nodes."
      ],
      "metadata": {
        "id": "qRwRRzkibbNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# result = df_large.join(broadcast(df_small), on=\"id\", how=\"inner\")\n",
        "\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "spark = SparkSession.builder.appName(\"BroadcastJoinExample\").getOrCreate()\n",
        "\n",
        "data_large = [(i, f\"Item_{i}\") for i in range(1, 1000001)]\n",
        "df_large = spark.createDataFrame(data_large, [\"id\", \"name\"])\n",
        "\n",
        "df_large.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMe4fOshbTN5",
        "outputId": "de54ca12-9513-4e9d-940a-984d9ee28055"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1| Item_1|\n",
            "|  2| Item_2|\n",
            "|  3| Item_3|\n",
            "|  4| Item_4|\n",
            "|  5| Item_5|\n",
            "|  6| Item_6|\n",
            "|  7| Item_7|\n",
            "|  8| Item_8|\n",
            "|  9| Item_9|\n",
            "| 10|Item_10|\n",
            "| 11|Item_11|\n",
            "| 12|Item_12|\n",
            "| 13|Item_13|\n",
            "| 14|Item_14|\n",
            "| 15|Item_15|\n",
            "| 16|Item_16|\n",
            "| 17|Item_17|\n",
            "| 18|Item_18|\n",
            "| 19|Item_19|\n",
            "| 20|Item_20|\n",
            "+---+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_small = [(1, \"Category_A\"), (2, \"Category_B\"), (3, \"Category_C\")]\n",
        "df_small = spark.createDataFrame(data_small, [\"id\", \"category\"])\n",
        "\n",
        "# Broadcast join\n",
        "joined_df = df_large.join(broadcast(df_small), on=\"id\", how=\"inner\")"
      ],
      "metadata": {
        "id": "w1yuVfkCbZfu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnAORpzHcukz",
        "outputId": "1d28f959-9398-41e6-d76c-d1b46e232b57"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+----------+\n",
            "| id|  name|  category|\n",
            "+---+------+----------+\n",
            "|  1|Item_1|Category_A|\n",
            "|  2|Item_2|Category_B|\n",
            "|  3|Item_3|Category_C|\n",
            "+---+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance Tips for Joins\n",
        "\n",
        "# 1. Use Broadcast Joins for Small Tables\n",
        "from pyspark.sql.functions import broadcast\n",
        "df_large.join(broadcast(df_small), on=\"id\", how=\"inner\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbON2EkZc8O0",
        "outputId": "41a0c62e-a0c3-4289-cb12-33b00454a45d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, name: string, category: string]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Repartition on Join Keys -- If both DataFrames are large, repartition them on the join key to reduce shuffle skew.\n",
        "\n",
        "df1 = df1.repartition(100, \"id\")\n",
        "df2 = df2.repartition(100, \"id\")\n",
        "joined_df = df1.join(df2, on=\"id\", how=\"inner\")"
      ],
      "metadata": {
        "id": "7zmAru8Yc8Mk"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Avoid Skew in Join Keys\n",
        "# 4. Cache or Persist Frequently Used Data\n",
        "\n",
        "df.cache()\n",
        "df.persist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R_wsRJac8Jw",
        "outputId": "841b4da4-cf44-4e2c-90e4-532fd7f2ecd6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[region: string, product: string, sales: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiU5uhc7d6iC",
        "outputId": "617a03a7-3d0b-4651-9839-a56ec0fd284e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----+\n",
            "|region| product|sales|\n",
            "+------+--------+-----+\n",
            "|  East|ProductA|  100|\n",
            "|  East|ProductB|  200|\n",
            "|  West|ProductA|  150|\n",
            "|  West|ProductB|  300|\n",
            "|  West|ProductA|  250|\n",
            "+------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Filter early\n",
        "df.filter(df.region=='East').show()\n",
        "df.select(\"product\").distinct().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Pud1ps8c8HM",
        "outputId": "3493b32d-fffd-4df1-974f-65c8aed7c723"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----+\n",
            "|region| product|sales|\n",
            "+------+--------+-----+\n",
            "|  East|ProductA|  100|\n",
            "|  East|ProductB|  200|\n",
            "+------+--------+-----+\n",
            "\n",
            "+--------+\n",
            "| product|\n",
            "+--------+\n",
            "|ProductB|\n",
            "|ProductA|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Prefer Equi-Joins over Cross Joins\n",
        "# 7. Optimize Join Type\n",
        "# 8. Adjust Spark Configurations for Large Joins\n",
        "\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 400) # default --> 200\n"
      ],
      "metadata": {
        "id": "svUUe9Mzc8EO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWPE2aoBep-_",
        "outputId": "2f36bb7b-8998-4406-c4ab-1ddd1f6df25b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----+\n",
            "|region| product|sales|\n",
            "+------+--------+-----+\n",
            "|  East|ProductA|  100|\n",
            "|  East|ProductB|  200|\n",
            "|  West|ProductA|  150|\n",
            "|  West|ProductB|  300|\n",
            "|  West|ProductA|  250|\n",
            "+------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Use Bucketing for Repeated Joins\n",
        "# For very large datasets joined frequently: Bucket both DataFrames by the same key and number of buckets.\n",
        "\n",
        "df.write.bucketBy(100, \"region\").saveAsTable(\"bucketed_table2\")"
      ],
      "metadata": {
        "id": "CraRud2rc8BS"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = spark.sql(\"select * from bucketed_table2\")\n",
        "res.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyLOZANPeoqH",
        "outputId": "c022469b-7a5d-46e1-db91-e31e7cfaf8bc"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----+\n",
            "|region| product|sales|\n",
            "+------+--------+-----+\n",
            "|  West|ProductA|  150|\n",
            "|  West|ProductB|  300|\n",
            "|  West|ProductA|  250|\n",
            "|  East|ProductA|  100|\n",
            "|  East|ProductB|  200|\n",
            "+------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Monitor Using Spark UI\n",
        "# After performing joins, check Spark UI → SQL tab → DAG"
      ],
      "metadata": {
        "id": "Ht9ncJabewr0"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Error Handling & Debugging in PySpark\n"
      ],
      "metadata": {
        "id": "mSlYvDa-fFPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sparkContext.setLogLevel(\"INFO\")   # or \"DEBUG\"  #  ERROR, WARN, INFO, DEBUG"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVXfHpNifCrI",
        "outputId": "20976c4b-3a97-4750-e6ea-6ee423a1e308"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[region: string, product: string, sales: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.limit(100).show()"
      ],
      "metadata": {
        "id": "7200RXTXsYN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "c1WKSxl8QuFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check null\n",
        "df.filter(col(\"id\").isNull()).show()"
      ],
      "metadata": {
        "id": "qjfBcE0HQuDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# changing log levels\n",
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "sc.setLogLevel(\"ERROR\")  # Options: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n"
      ],
      "metadata": {
        "id": "NpfvWkNtQt_K"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom logging\n",
        "import logging\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    format='%(asctime)s %(levelname)s %(message)s',\n",
        "    level=logging.INFO  # Set to DEBUG for more details\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Start Spark\n",
        "spark = SparkSession.builder.appName(\"LoggingExample\").getOrCreate()\n",
        "logger.info(\"Spark session started\")\n"
      ],
      "metadata": {
        "id": "2GWJAOLpdB_O"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = spark.range(10)\n",
        "logger.debug(f\"Schema: {df.schema}\")\n",
        "logger.info(f\"Row count: {df.count()}\")\n"
      ],
      "metadata": {
        "id": "65Lkp2RRp-b2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42ce66b0"
      },
      "source": [
        "You can check logger messages by looking at the standard output and standard error streams of your Spark application. The verbosity is controlled by the log level.\n",
        "\n",
        "Here's how you can set the log level and see the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd72a37b",
        "outputId": "38590f09-71e4-4982-f52e-0a6d52d41e2d"
      },
      "source": [
        "# Set the log level (e.g., to INFO to see more messages)\n",
        "spark.sparkContext.setLogLevel(\"INFO\")\n",
        "\n",
        "# You can then run your code, and messages at or above the INFO level will be printed.\n",
        "# For example, running the count operation again will show INFO level messages related to job execution.\n",
        "logger.info(\"Running count operation after setting log level to INFO\")\n",
        "df.count()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EBEtB7SyqUpG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}